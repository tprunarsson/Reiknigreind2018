{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Actor/Critic versus Table in Tic-Tac-Toe : TD($\\lambda$) - tabular versus function approximation\n",
    "This code illustrates how an after-state value function can be trained for the game tic-tac-toe using \"self-play\". The learning algorithm used is TD($\\lambda$) using after-states. One player uses a table to represent the after-state value function while the other player uses a neural network with one hidden layer (81 hidden nodes). The code also illustrates how pytorch may be used to train the neural network (using cuda or cpu) and auto-grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some game specific functions (check for winner, find legal moves and getotherplayer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function just works out the winner of the game\n",
    "def iswin(board, m):\n",
    "    if np.all(board[[0, 1, 2]] == m) | np.all(board[[3, 4, 5]] == m):\n",
    "        return 1\n",
    "    if np.all(board[[6, 7, 8]] == m) | np.all(board[[0, 3, 6]] == m):\n",
    "        return 1\n",
    "    if np.all(board[[1, 4, 7]] == m) | np.all(board[[2, 5, 8]] == m):\n",
    "        return 1\n",
    "    if np.all(board[[0, 4, 8]] == m) | np.all(board[[2, 4, 6]] == m):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# this function finds all legal actions (moves) for given state A(s)\n",
    "def legal_moves(board):\n",
    "    return np.where(board == 0)[0]\n",
    "\n",
    "# this function gets the other player in a turn taking game\n",
    "def getotherplayer(player):\n",
    "    if (player == 1):\n",
    "        return 2\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the functions needed by the table player to access the after-state value table quickly (hashit) and $\\epsilon$-greedy using the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is used to find an index to the after-state value table V(s)\n",
    "def hashit(board):\n",
    "    base3 = np.matmul(np.power(3, range(0, 9)), board.transpose())\n",
    "    return int(base3)\n",
    "\n",
    "# the usual epsilon greedy policy\n",
    "def epsilongreedy(board, player, epsilon, V, debug = False):\n",
    "    moves = legal_moves(board)\n",
    "    if (np.random.uniform() < epsilon):\n",
    "        if debug == True:\n",
    "            print(\"explorative move\")\n",
    "        return np.random.choice(moves, 1)\n",
    "    na = np.size(moves)\n",
    "    va = np.zeros(na)\n",
    "    for i in range(0, na):\n",
    "        board[moves[i]] = player\n",
    "        va[i] = V[hashit(board)]\n",
    "        board[moves[i]] = 0  # undo move\n",
    "    return moves[np.argmax(va)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define functions specific for the neural network representing the after-state value function. The first function defines how we encode the raw board for our neural network, we will use a common technique known as one-hot-encoding. A one hot encoding is a representation of categorical variables (1,2) as binary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is used to prepare the raw board as input to the network\n",
    "# for some games (not here) it may be useful to invert the board and see it from the perspective of \"player\"\n",
    "def one_hot_encoding(board, player):\n",
    "    one_hot = np.zeros( 2 * len(board) )\n",
    "    one_hot[np.where(board == 1)[0] ] = 1\n",
    "    one_hot[len(board) + np.where(board == 2)[0] ] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following softmax policy used a linear function approximator, the linear features are generated by the output layer of the neural network used by the Critic. Generating these features requires doing a forward sweep for the neural network for each possible board after-state. So here below, *x* is the input, this is multiplied by matrix *w1* and bias *b1* added followed by a squashing with the sigmoid function. This output *h_sigmoid* are the features used by the actor. This function also returns the weighted mean of these features, which is required for computing the gradient. The move selected is based on the softmax probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_policy(board, player, w1, b1, w2, b2, theta, debug = False):\n",
    "    moves = legal_moves(board)\n",
    "    na = np.size(moves)\n",
    "    PI = torch.zeros(na)\n",
    "    xs = np.zeros((theta.size(1),na))\n",
    "    for i in range(0, na):\n",
    "        board[moves[i]] = player\n",
    "        # encode the board to create the input\n",
    "        x = Variable(torch.tensor(one_hot_encoding(board, player), dtype = torch.float, device = device)).view(2*9,1)\n",
    "        # now do a forward pass to get the output layer's features\n",
    "        h = torch.mm(w1,x) + b1 # matrix-multiply x with input weight w1 and add bias\n",
    "        h_sigmoid = h.sigmoid() # squash this with a sigmoid function\n",
    "        pi = torch.mm(theta,h_sigmoid)\n",
    "        PI[i] = pi.item()\n",
    "        xs[:,i] = h_sigmoid.data.view(1,theta.size(1))\n",
    "    PI = PI.softmax(0)\n",
    "    xtheta_mean = torch.sum(torch.mm(torch.tensor(torch.from_numpy(xs), device = device, dtype = torch.float),torch.diag(PI.softmax(0))),1)\n",
    "    m = torch.multinomial(PI, 1)\n",
    "    return moves[m], xtheta_mean.view(theta.size(1),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main learning loop for the table versus neural network Actor-Critic. The table player will use the table while other player will be using the neural network with 81 hidden units. In this implementation the Actor uses the neural-network features discovered by the Critic. The actor updates only the parameter vector theta. It looks long and it may have been neater to implement the neural network as a class (as mentioned above). You may find it useful to look at this video: https://www.youtube.com/watch?v=ma2KXWblllc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learnit(numgames, epsilon, lam, alpha, V, alpha1, alpha2, w1, b1, w2, b2, alpha_th, theta):\n",
    "    gamma = 1 # for completeness\n",
    "    # play numgames games for training\n",
    "    for games in range(0, numgames):\n",
    "        board = np.zeros(9)    # initialize the board (empty)\n",
    "        # we will use TD(lambda) and so we need to use eligibility traces\n",
    "        S = [] # no after-state for table V, visited after-states is an empty list\n",
    "        E = np.array([]) # eligibility traces for table V\n",
    "        # now we initilize all the eligibility traces for the neural network\n",
    "        Z_w1 = torch.zeros(w1.size(), device = device, dtype = torch.float)\n",
    "        Z_b1 = torch.zeros(b1.size(), device = device, dtype = torch.float)\n",
    "        Z_w2 = torch.zeros(w2.size(), device = device, dtype = torch.float)\n",
    "        Z_b2 = torch.zeros(b2.size(), device = device, dtype = torch.float)\n",
    "        # player to start is \"1\" the other player is \"2\"\n",
    "        player = 1\n",
    "        tableplayer = 2\n",
    "        winner = 0 # this implies a draw\n",
    "        # start turn playing game, maximum 9 moves\n",
    "        for move in range(0, 9):\n",
    "            # use a policy to find action\n",
    "            if (player == tableplayer): # this one is using the table V\n",
    "                action = epsilongreedy(np.copy(board), player, epsilon, V)\n",
    "            else: # this one is using the neural-network to approximate the after-state value\n",
    "                action, xtheta = softmax_policy(np.copy(board), player, w1, b1, w2, b2, theta)\n",
    "            # perform move and update board\n",
    "            board[action] = player\n",
    "            if (1 == iswin(board, player)): # has this player won?\n",
    "                winner = player\n",
    "                break # bail out of inner game loop\n",
    "            # once both player have performed at least one move we can start doing updates\n",
    "            if (1 < move):\n",
    "                if tableplayer == player: # here we have player 1 updating the table V\n",
    "                    s = hashit(board) # get index to table for this new board\n",
    "                    delta = 0 + gamma * V[s] - V[sold]\n",
    "                    E = np.append(E,1) # add trace to this state (note all new states are unique else we would +1)\n",
    "                    S.append(sold)     # keep track of this state also\n",
    "                    V[S] = V[S] + delta * alpha * E # the usual tabular TD(lambda) update\n",
    "                    E = gamma * lam * E\n",
    "                else: # here we have player 2 updating the neural-network (2 layer feed forward with Sigmoid units)\n",
    "                    x = Variable(torch.tensor(one_hot_encoding(board, player), dtype = torch.float, device = device)).view(2*9,1)\n",
    "                    # now do a forward pass to evaluate the new board's after-state value\n",
    "                    h = torch.mm(w1,x) + b1 # matrix-multiply x with input weight w1 and add bias\n",
    "                    h_sigmoid = h.sigmoid() # squash this with a sigmoid function\n",
    "                    y = torch.mm(w2,h_sigmoid) + b2 # multiply with the output weights w2 and add bias\n",
    "                    y_sigmoid = y.sigmoid() # squash this with a sigmoid function\n",
    "                    target = y_sigmoid.detach().cpu().numpy()\n",
    "                    # lets also do a forward past for the old board, this is the state we will update\n",
    "                    h = torch.mm(w1,xold) + b1 # matrix-multiply x with input weight w1 and add bias\n",
    "                    h_sigmoid = h.sigmoid() # squash this with a sigmoid function\n",
    "                    y = torch.mm(w2,h_sigmoid) + b2 # multiply with the output weights w2 and add bias\n",
    "                    y_sigmoid = y.sigmoid() # squash the output\n",
    "                    delta2 = 0 + gamma * target - y_sigmoid.detach().cpu().numpy() # this is the usual TD error\n",
    "                    # using autograd and the contructed computational graph in pytorch compute all gradients\n",
    "                    y_sigmoid.backward()\n",
    "                    # update the eligibility traces using the gradients\n",
    "                    Z_w2 = gamma * lam * Z_w2 + w2.grad.data\n",
    "                    Z_b2 = gamma * lam * Z_b2 + b2.grad.data\n",
    "                    Z_w1 = gamma * lam * Z_w1 + w1.grad.data\n",
    "                    Z_b1 = gamma * lam * Z_b1 + b1.grad.data\n",
    "                    # zero the gradients\n",
    "                    w2.grad.data.zero_()\n",
    "                    b2.grad.data.zero_()\n",
    "                    w1.grad.data.zero_()\n",
    "                    b1.grad.data.zero_()\n",
    "                    # perform now the update for the weights\n",
    "                    delta2 =  torch.tensor(delta2, dtype = torch.float, device = device)\n",
    "                    w1.data = w1.data + alpha1 * delta2 * Z_w1\n",
    "                    b1.data = b1.data + alpha1 * delta2 * Z_b1\n",
    "                    w2.data = w2.data + alpha2 * delta2 * Z_w2\n",
    "                    b2.data = b2.data + alpha2 * delta2 * Z_b2\n",
    "                    # now perform the update for the Actor\n",
    "                    grad_ln_pi = h_sigmoid - xtheta\n",
    "                    theta.data = theta.data + alpha_th*delta2*grad_ln_pi.view(1,len(grad_ln_pi))\n",
    " \n",
    "\n",
    "            # we need to keep track of the last board state visited by the players\n",
    "            if tableplayer == player:\n",
    "                sold = hashit(board)\n",
    "            else:\n",
    "                xold = Variable(torch.tensor(one_hot_encoding(board, player), dtype=torch.float, device = device)).view(2*9,1)\n",
    "            # swap players\n",
    "            player = getotherplayer(player)\n",
    "\n",
    "        # The game epsiode has ended and we know the outcome of the game, and can find the terminal rewards\n",
    "        if winner == tableplayer:\n",
    "            reward = 0\n",
    "        elif winner == getotherplayer(tableplayer):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0.5\n",
    "        # Now we perform the final update (terminal after-state value is zero)\n",
    "        # these are basically the same updates as in the inner loop but for the final-after-states (sold and xold)\n",
    "        # first for the table (note if reward is 0 this player actually won!):\n",
    "        delta = (1.0 - reward) + gamma * 0 - V[sold]\n",
    "        E = np.append(E,1) # add one to the trace (recall unique states)\n",
    "        S.append(sold)\n",
    "        V[S] = V[S] + delta * alpha * E\n",
    "        # and then for the neural network:\n",
    "        h = torch.mm(w1,xold) + b1 # matrix-multiply x with input weight w1 and add bias\n",
    "        h_sigmoid = h.sigmoid() # squash this with a sigmoid function\n",
    "        y = torch.mm(w2,h_sigmoid) + b2 # multiply with the output weights w2 and add bias\n",
    "        y_sigmoid = y.sigmoid() # squash the output\n",
    "        delta2 = reward + gamma * 0 - y_sigmoid.detach().cpu().numpy()  # this is the usual TD error\n",
    "        # using autograd and the contructed computational graph in pytorch compute all gradients\n",
    "        y_sigmoid.backward()\n",
    "        # update the eligibility traces\n",
    "        Z_w2 = gamma * lam * Z_w2 + w2.grad.data\n",
    "        Z_b2 = gamma * lam * Z_b2 + b2.grad.data\n",
    "        Z_w1 = gamma * lam * Z_w1 + w1.grad.data\n",
    "        Z_b1 = gamma * lam * Z_b1 + b1.grad.data\n",
    "        # zero the gradients\n",
    "        w2.grad.data.zero_()\n",
    "        b2.grad.data.zero_()\n",
    "        w1.grad.data.zero_()\n",
    "        b1.grad.data.zero_()\n",
    "        # perform now the update of weights\n",
    "        delta2 =  torch.tensor(delta2, dtype = torch.float, device = device)\n",
    "        w1.data = w1.data + alpha1 * delta2 * Z_w1\n",
    "        b1.data = b1.data + alpha1 * delta2 * Z_b1\n",
    "        w2.data = w2.data + alpha2 * delta2 * Z_w2\n",
    "        b2.data = b2.data + alpha2 * delta2 * Z_b2\n",
    "        # now perform the update for the Actor\n",
    "        grad_ln_pi = h_sigmoid - xtheta\n",
    "        theta.data = theta.data + alpha_th*delta2*grad_ln_pi.view(1,len(grad_ln_pi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the main part of the code the calls the learning procedure, here you can also choose to use cuda or cpu. Note that the choice of alpha is critical for success aswell as the training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.0385386943817\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# cuda will only create a significant speedup for large/deep networks and batched training\n",
    "# device = torch.device('cuda') \n",
    "\n",
    "# global after-state value function, note this table is too big and contrains states that\n",
    "# will never be used, also each state is unique to the player (no one after-state seen by both players)\n",
    "V = np.zeros(hashit(2 * np.ones(9)))\n",
    "\n",
    "alpha = 0.1 # step size for tabular learning\n",
    "alpha1 = 0.1 # step sizes using for the neural network (first layer)\n",
    "alpha2 = 0.1 # (second layer), the critic's neural network\n",
    "alpha_th = 0.001 # for the Actor's thetas, uses softmax policy\n",
    "epsilon = 0.1 # exploration parameter used by epsilon greedy player (table)\n",
    "lam = 0.4 # lambda parameter in TD(lam-bda)\n",
    "\n",
    "# define the parameters for the single hidden layer feed forward neural network\n",
    "# randomly initialized weights with zeros for the biases\n",
    "w1 = Variable(torch.randn(9*9,2*9, device = device, dtype=torch.float), requires_grad = True)\n",
    "b1 = Variable(torch.zeros((9*9,1), device = device, dtype=torch.float), requires_grad = True)\n",
    "w2 = Variable(torch.randn(1,9*9, device = device, dtype=torch.float), requires_grad = True)\n",
    "b2 = Variable(torch.zeros((1,1), device = device, dtype=torch.float), requires_grad = True)\n",
    "\n",
    "# here I have added the linear weights for the Actor\n",
    "theta = 0.001*torch.ones((1,9*9), device = device, dtype=torch.float)\n",
    "\n",
    "# now perform the actual training and display the computation time\n",
    "import time\n",
    "start = time.time()\n",
    "training_steps = 40000\n",
    "learnit(training_steps, epsilon, lam, alpha, V, alpha1, alpha2, w1, b2, w2, b2, alpha_th, theta)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we should play 100 games against another opponent, never seen before but here, for now, we will just see how these two players perform against each other, note epsilon is still at its training value, this will create a greater variation of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def competition(V, w1, b1, w2, b2, theta, epsilon = 0.0, debug = False):\n",
    "    board = np.zeros(9)          # initialize the board\n",
    "    # player to start is \"1\" the other player is \"2\"\n",
    "    player = 1\n",
    "    tableplayer = 2\n",
    "    winner = 0 # default draw\n",
    "    # start turn playing game, maximum 9 moves\n",
    "    for move in range(0, 9):\n",
    "        # use a policy to find action, switch off exploration\n",
    "        if (tableplayer == player):\n",
    "            action = epsilongreedy(np.copy(board), player, epsilon, V, debug)\n",
    "        else:\n",
    "            action, _ = softmax_policy(np.copy(board), player, w1, b1, w2, b2, theta)\n",
    "        # perform move and update board (for other player)\n",
    "        board[action] = player\n",
    "        if debug: # print the board, when in debug mode\n",
    "            symbols = np.array([\" \", \"X\", \"O\"])\n",
    "            print(\"player \", symbols[player], \", move number \", move+1, \":\", action)\n",
    "            print(symbols[board.astype(int)].reshape(3,3))\n",
    "\n",
    "        if (1 == iswin(board, player)): # has this player won?\n",
    "            winner = player\n",
    "            break\n",
    "        player = getotherplayer(player) # swap players\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let the player compete for 100 games (using their $\\epsilon$-greedy policy) and then play one deterministic game with $\\epsilon=0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0 33.0 35.0\n",
      "player  X , move number  1 : 2\n",
      "[[' ' ' ' 'X']\n",
      " [' ' ' ' ' ']\n",
      " [' ' ' ' ' ']]\n",
      "player  O , move number  2 : 0\n",
      "[['O' ' ' 'X']\n",
      " [' ' ' ' ' ']\n",
      " [' ' ' ' ' ']]\n",
      "player  X , move number  3 : 4\n",
      "[['O' ' ' 'X']\n",
      " [' ' 'X' ' ']\n",
      " [' ' ' ' ' ']]\n",
      "player  O , move number  4 : 6\n",
      "[['O' ' ' 'X']\n",
      " [' ' 'X' ' ']\n",
      " ['O' ' ' ' ']]\n",
      "player  X , move number  5 : 3\n",
      "[['O' ' ' 'X']\n",
      " ['X' 'X' ' ']\n",
      " ['O' ' ' ' ']]\n",
      "player  O , move number  6 : 5\n",
      "[['O' ' ' 'X']\n",
      " ['X' 'X' 'O']\n",
      " ['O' ' ' ' ']]\n",
      "player  X , move number  7 : 1\n",
      "[['O' 'X' 'X']\n",
      " ['X' 'X' 'O']\n",
      " ['O' ' ' ' ']]\n",
      "player  O , move number  8 : 7\n",
      "[['O' 'X' 'X']\n",
      " ['X' 'X' 'O']\n",
      " ['O' 'O' ' ']]\n",
      "player  X , move number  9 : 8\n",
      "[['O' 'X' 'X']\n",
      " ['X' 'X' 'O']\n",
      " ['O' 'O' 'X']]\n"
     ]
    }
   ],
   "source": [
    "wins_for_player_1 = 0\n",
    "draw_for_players = 0\n",
    "loss_for_player_1 = 0\n",
    "competition_games = 100\n",
    "for j in range(competition_games):\n",
    "    winner = competition(V, w1, b1, w2, b2, theta, epsilon, debug = False)\n",
    "    if (winner == 1):\n",
    "        wins_for_player_1 += 1.0\n",
    "    elif (winner == 0):\n",
    "        draw_for_players += 1.0\n",
    "    else:\n",
    "        loss_for_player_1 += 1.0\n",
    "\n",
    "print(wins_for_player_1, draw_for_players, loss_for_player_1)\n",
    "# lets also play one deterministic games:\n",
    "winner = competition(V, w1, b1, w2, b2, theta, 0, debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
